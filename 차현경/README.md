## 차현경📝

### Apache Spark

    - 대용량 데이터 처리를 위한 클러스터 컴퓨팅 프레임워크
    - 하둡 생태계를 보완하는 기술로 기동할 때 하둡의 기능들을 사용한다.

### Cluster

    - 여러 대의 컴퓨터들이 연결되어 하나의 시스템처럼 동작하는 컴퓨터들의 집합
    - 데이터베이스에서 여러 개의 서버가 하나의 데이터베이스를 나눠서 처리하는 형태
    - 고가용성, 병렬처리, 성능향상

### Spark Cluster

    - Master node와 Worker node로 구성된다.
    - 구성 요소
        - Master node
            - 전체 클러스터를 관리하고 분석 프로그램을 수행하는 역할
            - 분석 프로그램을 스파크 클러스터에 실행하면 하나의 JOB이 생성된다.
            - 드라이버 노드
                - 사용자가 만든 분석 프로그램
                - 사용자의 main 메소드가 실행되는 프로세스
                - 사용자 프로그램을 task로 변환하여 클러스터로 전송
                - executor에서의 개별 작업들을 위한 스케줄링을 조정
        - Worker node
            - 개별 task를 실행하는 작업 실행 프로세스
            - task 실행 후 결과를 드라이버로 전송
            - 사용자 프로그램에서 캐시하는 RDD를 저장하기 위한 메모리 공간 제공
        - 클러스터 매니저
            - Executor를 실행하기 위해 클러스트 매니저에 의존
            - Standalone, Hadoop Yarn, Apache Mesos

    - 프로그램 실행 단계
        1. 사용자가 spark-submit을 사용해 애플리케이션 제출
        2. spark-submit은 드라이버 프로그램을 실행하여 main 메소드 호출
        3. 드라이버는 클러스터 매니저에서 익스큐터 실행을 위한 리소스 요청
        4. 클러스터 매니저는 익스큐터를 실행
        5. 드라이버는 태스크 단위로 나누어 익스큐터에 전송
        6. 익스큐터는 태스크를 실행
        7. 애플리케이션이 종료되면 클러스터 매니저에게 리소스 반납

### SparkSession

    - 스파크 응용 프로그램의 통합 진입점
    - Build Pattern을 사용해서 생성
    - spark2.x 이후에 모든 Spark 기능의 진입점
    - SparkContext에서 사용할 수 있는 기능은 모두 SparkSession에서 사용 가능
        - spark.sparkContext : 애플리케이션 전체의 실행 관련 정보 집약 객체로 스케줄러 등이 포함 (RDD 생성 시)
        - spark.sparkSession : 스파크 컨텍스트에 세션 정보가 추가로 포함 (DataFrame 생성 시)
